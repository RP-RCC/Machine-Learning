# üìä APLICACI√ìN DE APRENDIZAJE AUTOM√ÅTICO PARA EL PRON√ìSTICO DE LLAMADAS ENTRANTES DESDE CABINAS DE TELEPRESENCIA 

Este repositorio contiene el desarrollo de un modelo predictivo para anticipar el volumen de llamadas entrantes desde **cabinas de telepresencia** en centros de atenci√≥n al cliente de una empresa de telecomunicaciones en Per√∫. Se utilizaron t√©cnicas de aprendizaje autom√°tico supervisado aplicadas a datos correspondientes del periodo **abril 2024 - abril 2025**.

---

## üéØ Objetivo del Proyecto

Predecir el n√∫mero de llamadas entrantes desde cabinas de telepresencia ubicadas en centros de atenci√≥n al cliente de una empresa de telecomunicaciones basado en las llamadas del 2024 y 2025 utilizando modelos de aprendizaje autom√°tico.

---

## üß† Modelos de Machine Learning Utilizados

Se evaluaron varios modelos de regresi√≥n:

- ‚úÖ **Random Forest**
- ‚úÖ **Gradient Boosting Regressor (GBR)**
- ‚úÖ **XGBoost**
- ‚úÖ **LightGBM (LGBM)** ‚Äî *Mejor desempe√±o*

> Se aplic√≥ la metodolog√≠a **CRISP-DM** para un proceso estructurado y reproducible.
---

## üß† Protocolo de experimentaci√≥n

Se evaluaron 4 modelos bajo un protocolo de experimentaci√≥n.

### Detalle de Hiperpar√°metros y Experimentaciones

#### Random Forest
| Hiperpar√°metro            | Valores                       | N¬∫ Experimentos |
|---------------------------|-------------------------------|-----------------|
| N√∫mero de √°rboles         | 20, 30, 50, 100               | 4               |
| Max Features              | 0.01, 0.1, 0.3, 0.5, 1        | 5               |
| N¬∫ muestras por nodo      | 6, 7, 8, 9, 10                | 5               |
| **Total de experimentaciones** |                               | **100**         |

#### XGBoost
| Hiperpar√°metro            | Valores                       | N¬∫ Experimentos |
|---------------------------|-------------------------------|-----------------|
| Profundidad m√°xima        | 3, 5, 7, 9                    | 4               |
| Tasa de aprendizaje       | 0.01, 0.001, 0.0001           | 3               |
| Subsample                 | 0.8, 0.9, 1                   | 3               |
| N¬∫ de iteraciones         | 1000                          | 1               |
| N¬∫ de n√∫cleos             | 2                             | 1               |
| Funci√≥n de p√©rdida        | reg:squarederror              | 1               |
| Lambda                    | 1, 2                          | 2               |
| Alpha                     | 0, 0.5                        | 2               |
| **Total de experimentaciones** |                               | **144**         |

#### Gradient Boosting Regressor
| Hiperpar√°metro            | Valores                       | N¬∫ Experimentos |
|---------------------------|-------------------------------|-----------------|
| N√∫mero de √°rboles         | 100, 300, 500                 | 3               |
| Tasa de aprendizaje       | 0.001, 0.01, 0.05             | 3               |
| Profundidad de √°rbol      | 3, 5, 7, 9                    | 4               |
| Min_samples_split         | 10                            | 1               |
| Min_samples_leaf          | 5                             | 1               |
| Subsample                 | 0.8, 1                        | 2               |
| max_features              | 0.8, 1                        | 2               |
| **Total de experimentaciones** |                               | **144**         |

#### LightGBM
| Hiperpar√°metro                    | Valores                       | N¬∫ Experimentos |
|-----------------------------------|-------------------------------|-----------------|
| N√∫mero de √°rboles                 | 100, 300, 500                 | 3               |
| N√∫mero m√°ximo de hojas            | 31, 50                        | 2               |
| Tasa de aprendizaje               | 0.01, 0.05, 0.1               | 3               |
| Profundidad de √°rbol              | -1, 5, 10                     | 3               |
| M√≠nimo n√∫mero de muestras por hoja| 1, 20                         | 2               |
| Objective                         | regression                    | 1               |
| Tipo de boosting                  | gbdt                          | 1               |
| **Total de experimentaciones**     |                               | **108**         |



### üî¢ **Total general de experimentaciones: 496**
---

## üìà Principales Resultados (Test)

| Modelo        | R¬≤ Score | RMSE   | MAE    | MAPE (%) |
|---------------|----------|--------|--------|----------|
| LightGBM      | 0.9985   | 0.1250 | 0.0283 | 0.5345   |
| XGBoost       | 0.7111   | 2.0783 | 1.4451 | 51.0218  |
| GBR           | 0.7132   | 2.0708 | 1.4409 | 50.7703  |
| Random Forest | 0.9960   | 0.2059 | 0.0264 | 0.5388   |

> El modelo LightGBM fue el m√°s preciso, permitiendo predicciones confiables por rango horario.

---

## üß∞ Herramientas y Tecnolog√≠as

- Python 3.x
- Scikit-learn
- LightGBM
- XGBoost
- Pandas / NumPy
- Seaborn / Matplotlib


---

## üìå Variables Consideradas
- Temporales: A√±o, mes, d√≠a, semana, feriado, fin de semana, rango horario, d√≠a laboral, estaci√≥n, semana2, semana3.
- Geogr√°ficas: Oficina, regi√≥n,canal.
- Atenci√≥n: Tipo de atenci√≥n, promedios hist√≥ricos.



