# üìä APLICACI√ìN DE APRENDIZAJE AUTOM√ÅTICO PARA EL PRON√ìSTICO DE LLAMADAS ENTRANTES DESDE CABINAS DE TELEPRESENCIA 

Este repositorio contiene el desarrollo de un modelo predictivo para anticipar el volumen de llamadas entrantes desde **cabinas de telepresencia** en centros de atenci√≥n al cliente de una empresa de telecomunicaciones en Per√∫. Se utilizaron t√©cnicas de aprendizaje autom√°tico supervisado aplicadas a datos correspondientes del periodo **abril 2024 - abril 2025**.

---

## üéØ Objetivo del Proyecto

Predecir el n√∫mero de llamadas entrantes desde cabinas de telepresencia ubicadas en centros de atenci√≥n al cliente de una empresa de telecomunicaciones basado en las llamadas del 2024 y 2025 utilizando modelos de aprendizaje autom√°tico.

---

## üß† Modelos de Machine Learning Utilizados

Se evaluaron varios modelos de regresi√≥n:

- ‚úÖ **Random Forest**
- ‚úÖ **Gradient Boosting Regressor (GBR)**
- ‚úÖ **XGBoost**
- ‚úÖ **LightGBM (LGBM)** ‚Äî *Mejor desempe√±o*

> Se aplic√≥ la metodolog√≠a **CRISP-DM** para un proceso estructurado y reproducible.
---


## üìå Variables Consideradas
- Temporales: A√±o, mes, d√≠a, semana, feriado, fin de semana, rango horario, d√≠a laboral, estaci√≥n, semana2, semana3.
- Geogr√°ficas: Oficina, regi√≥n,canal.
- Atenci√≥n: Tipo de atenci√≥n, promedios hist√≥ricos.

---

## üî¢ Protocolo de experimentaci√≥n

Se evaluaron 4 modelos bajo un protocolo de experimentaci√≥n.

### Detalle de Hiperpar√°metros y Experimentaciones

#### Random Forest
| Hiperpar√°metro            | Valores                       | N¬∫ Experimentos |
|---------------------------|-------------------------------|-----------------|
| N√∫mero de √°rboles         | 20, 30, 50, 100               | 4               |
| Max Features              | 0.01, 0.1, 0.3, 0.5, 1        | 5               |
| N¬∫ muestras por nodo      | 6, 7, 8, 9, 10                | 5               |
| **Total de experimentaciones** |                               | **100**         |

#### XGBoost
| Hiperpar√°metro            | Valores                       | N¬∫ Experimentos |
|---------------------------|-------------------------------|-----------------|
| Profundidad m√°xima        | 3, 5, 7, 9                    | 4               |
| Tasa de aprendizaje       | 0.01, 0.001, 0.0001           | 3               |
| Subsample                 | 0.8, 0.9, 1                   | 3               |
| N¬∫ de iteraciones         | 1000                          | 1               |
| N¬∫ de n√∫cleos             | 2                             | 1               |
| Funci√≥n de p√©rdida        | reg:squarederror              | 1               |
| Lambda                    | 1, 2                          | 2               |
| Alpha                     | 0, 0.5                        | 2               |
| **Total de experimentaciones** |                               | **144**         |

#### Gradient Boosting Regressor
| Hiperpar√°metro            | Valores                       | N¬∫ Experimentos |
|---------------------------|-------------------------------|-----------------|
| N√∫mero de √°rboles         | 100, 300, 500                 | 3               |
| Tasa de aprendizaje       | 0.001, 0.01, 0.05             | 3               |
| Profundidad de √°rbol      | 3, 5, 7, 9                    | 4               |
| Min_samples_split         | 10                            | 1               |
| Min_samples_leaf          | 5                             | 1               |
| Subsample                 | 0.8, 1                        | 2               |
| max_features              | 0.8, 1                        | 2               |
| **Total de experimentaciones** |                               | **144**         |

#### LightGBM
| Hiperpar√°metro                    | Valores                       | N¬∫ Experimentos |
|-----------------------------------|-------------------------------|-----------------|
| N√∫mero de √°rboles                 | 100, 300, 500                 | 3               |
| N√∫mero m√°ximo de hojas            | 31, 50                        | 2               |
| Tasa de aprendizaje               | 0.01, 0.05, 0.1               | 3               |
| Profundidad de √°rbol              | -1, 5, 10                     | 3               |
| M√≠nimo n√∫mero de muestras por hoja| 1, 20                         | 2               |
| Objective                         | regression                    | 1               |
| Tipo de boosting                  | gbdt                          | 1               |
| **Total de experimentaciones**     |                               | **108**         |

> **Total general de experimentaciones: 496**
---

## üìà Principales Resultados (Test)

| Modelo        | R¬≤ Score | RMSE   | MAE    | MAPE (%) |
|---------------|----------|--------|--------|----------|
| LightGBM      | 0.9985   | 0.1250 | 0.0283 | 0.5345   |
| XGBoost       | 0.7111   | 2.0783 | 1.4451 | 51.0218  |
| GBR           | 0.7132   | 2.0708 | 1.4409 | 50.7703  |
| Random Forest | 0.9960   | 0.2059 | 0.0264 | 0.5388   |

 El modelo LightGBM fue el m√°s preciso, permitiendo predicciones confiables por rango horario. Los hiperpar√°metros del modelo de LGBM fueron:
- n_estimators: 500
- num_leaves:	50
- learning_rate:	0.1
- max_depth:	-1
- min_child_samples:	20
- objective:	regression
- boosting_type:	gbdt


---

## üß∞ Herramientas y Tecnolog√≠as

- Python 3.x
- Scikit-learn
- LightGBM
- XGBoost
- Pandas / NumPy
- Seaborn / Matplotlib




---
## üìå Conclusiones

Los modelos de aprendizaje autom√°tico lograron predecir el n√∫mero de llamadas entrantes desde cabinas de telepresencia ubicadas en centros de atenci√≥n al cliente de una empresa de telecomunicaciones basado en las llamadas del 2024 y 2025.

La exploraci√≥n y preparaci√≥n del conjunto de datos hist√≥ricos de llamadas entrantes del 2024 y 2025 contribuyeron a la predicci√≥n del n√∫mero de llamadas entrantes desde cabinas de telepresencia ya que proporcionan datos procesados y listos para usar en modelos de machine learning. 

La aplicaci√≥n de los modelos Random Forest, XGBoost, GBR y LGBM contribuyeron a la predicci√≥n del n√∫mero de llamadas entrantes desde cabinas de telepresencia ya que brindan diferentes predicciones para una posterior comparaci√≥n.

El modelo LightGBM tuvo los mejores resultados R^2  y RMSE en la predicci√≥n del n√∫mero de llamadas entrantes desde cabinas de telepresencia.

---
## üìå Recomendaciones

Adicionar m√°s variables derivadas (engineered features) y evaluar si mejora el resultado de los indicadores ya que, aunque el modelo ha demostrado una capacidad predictiva sobresaliente, es recomendable continuar explorando y creando variables derivadas que puedan capturar patrones m√°s profundos en los datos.

Adicionar variables externas y evaluar el impacto que tendr√≠an en el desempe√±o del modelo debido a que actualmente, el modelo se basa en variables internas del sistema (d√≠a, canal, tipo de atenci√≥n, etc.), pero se sugiere incluir variables externas que puedan afectar la demanda de llamadas y que no est√°n contenidas en el sistema, como condiciones clim√°ticas, eventos relevantes, cortes de servicio previos o mantenimiento, etc.

Evaluar modelos adicionales encontrados en el estado del arte como GRU y SARIMA pues si bien los modelos basados en √°rboles (LightGBM, XGBoost, Random Forest) han ofrecido excelentes resultados, es importante contrastarlos con enfoques alternativos, especialmente aquellos orientados a series temporales.

Ampliar el periodo de recolecci√≥n de datos dado que el dataset cubre un a√±o (abril 2024 - abril 2025), se recomienda extender la cobertura a m√∫ltiples a√±os para identificar patrones multianuales, efectos post-pandemia o cambios por estacionalidades espec√≠ficas.

Una vez implementado, el modelo debe ser monitoreado mensualmente. Si la demanda cambia, el modelo debe reentrenarse.

